{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Initiating a `SparkSession`\n",
    "\n",
    "1\\. Initiate a `SparkSession`. A `SparkSession` initializes both a `SparkContext` and a `SQLContext` to use RDD-based and DataFrame-based functionalities of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"df lecture\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "sc = spark.sparkContext\n",
    "sq = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to SparkSQL\n",
    "\n",
    "SparkSQL allows you to execute relational queries on **structured** data using \n",
    "Spark. Today we'll get some practice with this by running some queries on a \n",
    "Yelp dataset. To begin, you will load data into a Spark `DataFrame`, which can \n",
    "then be queried as a SQL table. \n",
    "\n",
    "1\\. Load the Yelp business data using the function `.read.json()` from the `SparkSession()` object, with input file `data/yelp_academic_dataset_business.json.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=spark.read.json('/Users/datascientist/Downloads/spark_files/data/yelp_academic_dataset_business.json.gz')\n",
    "\n",
    "df.registerTempTable(\"yelp_business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Print the schema and register the `yelp_business_df` as a temporary \n",
    "table named `yelp_business` (this will enable us to query the table later using \n",
    "our `SparkSession()` object).\n",
    "\n",
    "Now, you can run SQL queries on the `yelp_business` table. For example:\n",
    "\n",
    "```python\n",
    "result = spark.sql(\"SELECT name, city, state, stars FROM yelp_business LIMIT 10\")\n",
    "result.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          categories|\n",
      "+--------------------+\n",
      "|[Doctors, Health ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT categories FROM yelp_business LIMIT 1\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Write a query or a sequence of transformations that returns the `name` of entries that fulfill the following \n",
    "conditions:\n",
    "\n",
    "   - Rated at 5 `stars`\n",
    "   - In the `city` of Phoenix\n",
    "   - Accepts credit card (Reference the `'Accepts Credit Card'` field by \n",
    "   ``` attributes.`Accepts Credit Cards` ```)\n",
    "   - Contains Restaurants in the `categories` array.  \n",
    "\n",
    "   Hint: `LATERAL VIEW explode()` can be used to access the individual elements\n",
    "   of an array (i.e. the `categories` array). For reference, you can see the \n",
    "   [first example](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView) on this page.\n",
    "   \n",
    "   Hint: In spark, while using `filter()` or `where()`, you can create a condition that tests if a column, made of an array, contains a given value. The functions is [pyspark.sql.functions.array_contains](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.array_contains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<categories LIKE Restaurants>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.categories.like('Restaurants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+\n",
      "|                name|   city|          categories|          attributes|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "|       Auslers Grill|Phoenix|       [Restaurants]|[true,null,null,f...|\n",
      "|Mulligan's Restau...|Phoenix|       [Restaurants]|[true,null,null,b...|\n",
      "|             Sunfare|Phoenix|[Food Delivery Se...|[true,null,null,n...|\n",
      "|              Subway|Phoenix|[Fast Food, Sandw...|[true,null,null,n...|\n",
      "|           Lil Cal's|Phoenix|       [Restaurants]|[true,null,null,f...|\n",
      "|                Ed's|Phoenix|[American (Tradit...|[true,null,null,f...|\n",
      "|Frenchys Caribbea...|Phoenix|[Food, Hot Dogs, ...|[true,null,null,n...|\n",
      "|           WY Market|Phoenix|[American (Tradit...|[true,null,null,b...|\n",
      "|       Pollo Sabroso|Phoenix|[Fast Food, Ameri...|[true,null,null,n...|\n",
      "|Queen Creek Olive...|Phoenix|[Food, Specialty ...|[true,null,null,b...|\n",
      "|Gluten Free Creat...|Phoenix|[Bakeries, Food, ...|[true,null,null,n...|\n",
      "|Panini Bread and ...|Phoenix|[American (Tradit...|[true,null,null,n...|\n",
      "|        One Eighty Q|Phoenix|[Food, Barbeque, ...|[true,null,null,n...|\n",
      "|Saffron JAK Origi...|Phoenix|[Food, Pizza, Foo...|[true,null,null,n...|\n",
      "|Los Primos Carnic...|Phoenix|[Mexican, Restaur...|[true,null,null,n...|\n",
      "| Bertie's Of Arcadia|Phoenix|[Soup, Comfort Fo...|[true,null,null,n...|\n",
      "|     Little Miss BBQ|Phoenix|[Barbeque, Restau...|[true,null,null,n...|\n",
      "|Las Jicaras Mexic...|Phoenix|[Mexican, Restaur...|[true,null,null,n...|\n",
      "|  Santos Lucha Libre|Phoenix|[Mexican, Restaur...|[true,null,null,n...|\n",
      "|   Taqueria El Chino|Phoenix|[Mexican, Restaur...|[true,null,null,n...|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT name, city, categories, attributes FROM yelp_business LATERAL VIEW explode(categories) as cat WHERE stars = '5.0' AND city = 'Phoenix' AND attributes.`Accepts Credit Cards` = 'true' AND cat='Restaurants'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Spark and SparkSQL in Practice \n",
    "\n",
    "Now that we have a basic knowledge of how SparkSQL works, let's try dealing with a real-life scenario where some data manipulation/cleaning is required before we can query the data with SparkSQL. We will be using a dataset of user information and a data set of purchases that our users have made. We'll be cleaning the data in a regular Spark RDD before querying it with SparkSQL.\n",
    "\n",
    "   1\\. Load a dataframe `users` from S3 link `''s3a://sparkdatasets/users.txt'` (no credentials needed but if you en\n",
    "   counter any problem just us\n",
    "   \n",
    "   e local copy `data/users.txt` instead) using `spark.read.csv` with the following parameters: no headers, use separator `\";\"`, and infer the schema of the underlying data (for now). Use `.show(5)` and `.printSchema()` to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users=spark.read.csv('../data/users.txt',header=False,sep=';',inferSchema=True)\n",
    "users.take(10)\n",
    "type(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2\\. Create a schema for this dataset using proper names and types for the columns, using types from the `pyspark.sql.types` module (see lecture). Use that schema to read the `users` dataframe again and use `.printSchema()` to check the result.\n",
    "   \n",
    "   Note: Each row in the `users` file represents the user with his/her `user_id, name, email, phone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = users.toDF('id','name','email','phone')\n",
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|        id|                name|               email|               phone|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|1106214172|   Prometheus Barwis|prometheus.barwis...|      (533) 072-2779|\n",
      "| 527133132|   Ashraf Bainbridge|ashraf.bainbridge...|                null|\n",
      "|1290614884|      Alain Hennesey|alain.hennesey@fa...|(942) 208-8460,(8...|\n",
      "|1700818057|    Hamed Fingerhuth|hamed.fingerhuth@...|                null|\n",
      "|  17378782|       Annamae Leyte|annamae.leyte@msn...|                null|\n",
      "|1723254379|         Chao Peachy|chao.peachy@me.co...|      (510) 121-0098|\n",
      "|1946358537|Somtochukwu Mouri...|somtochukwu.mouri...|      (669) 504-8080|\n",
      "|  33663453|     Elisabeth Berry|elisabeth.berry@f...|      (802) 973-8267|\n",
      "|1329323232|       Jalan Blakely|jalan.blakely@gma...|                null|\n",
      "|  68524725|         Lyric Boddy|lyric.boddy@yahoo...|      (273) 077-4039|\n",
      "| 629898066| Emilygrace Bossence|emilygrace.bossen...|                null|\n",
      "|1980300225|         Warner Eddy|warner.eddy@gmail...|(213) 586-6234,(6...|\n",
      "|1044067626|     Kienan Drummond|kienan.drummond@a...|(112) 595-9033,(0...|\n",
      "|1880278862|  Zamirah Schedewick|zamirah.schedewic...|      (796) 133-2849|\n",
      "| 590040358|      Makel Woodgate|makel.woodgate@me...|(132) 622-9301,(5...|\n",
      "|1616169115|     Reetal Robinson|reetal.robinson@a...|      (812) 353-1872|\n",
      "| 344531853|     Jayceona Callan|jayceona.callan@y...|      (823) 005-9613|\n",
      "|1971119589|    Goddess Chadwick|goddess.chadwick@...|      (755) 651-6721|\n",
      "|1057344268|        Sanari Wedge|sanari.wedge@face...|                null|\n",
      "| 928957074|       Ryden Stinson|ryden.stinson@yah...|                null|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   3\\. Load an RDD `transactions_rdd` from S3 link `''s3a://sparkdatasets/transactions.txt'` (no credentials needed but if you encounter any problem just use local copy `data/transactions.txt` instead) using `spark.sparkContext.textFile`. Use `.take(5)` to check the result.\n",
    "   \n",
    "   Use `.map()` to split those csv-like lines, to strip the dollar sign on the second column, and to cast each column to its proper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|       _c0|    _c1|                 _c2|\n",
      "+----------+-------+--------------------+\n",
      "| 815581247|$144.82|2015-09-05 00:00:...|\n",
      "|1534673027|$140.93|2014-03-11 00:00:...|\n",
      "| 842468364|$104.26|2014-05-06 00:00:...|\n",
      "|1720001139|$194.60|2015-08-24 00:00:...|\n",
      "|1397891675|$307.72|2015-09-25 00:00:...|\n",
      "| 926282663| $36.69|2014-10-24 00:00:...|\n",
      "| 694853136| $39.59|2014-11-26 00:00:...|\n",
      "| 636287877|$430.94|2015-06-12 00:00:...|\n",
      "|1396310477|  $31.4|2014-12-05 00:00:...|\n",
      "|1279939289|$180.69|2015-03-26 00:00:...|\n",
      "| 859061953|$383.35|2014-06-06 00:00:...|\n",
      "|1983919868| $256.2|2015-09-28 00:00:...|\n",
      "| 589339046|$930.56|2014-09-21 00:00:...|\n",
      "|1559785598|$423.77|2015-05-18 00:00:...|\n",
      "| 347589978|$309.53|2015-10-11 00:00:...|\n",
      "| 963722938|$299.19|2014-04-06 00:00:...|\n",
      "|1808365853|$426.21|2015-09-10 00:00:...|\n",
      "| 417552135|$732.27|2015-09-30 00:00:...|\n",
      "| 744965566|$186.33|2015-12-30 00:00:...|\n",
      "|1513020241| $925.8|2014-10-06 00:00:...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions=spark.read.csv('../data/transactions.txt',header=False,sep=';',inferSchema=True)\n",
    "transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4\\. Create a schema for this dataset using proper names and types for the columns, using types from the `pyspark.sql.types` module (see lecture). Use that schema to convert `transactions_rdd` into a dataframe `transactions`  and use `.show(5)` and `.printSchema()` to check the result.\n",
    "   \n",
    "   Each row in the `transactions` file has the columns  `user_id, amount_paid, date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- amount_paid: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = transactions.toDF('id','amount_paid','date')\n",
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Write a sequence of transformations or a SQL query that returns the names and the amount paid for the users with the **top 10** transaction amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+----------+-----------------+--------------------+--------------------+\n",
      "|        id|amount_paid|                date|        id|             name|               email|               phone|\n",
      "+----------+-----------+--------------------+----------+-----------------+--------------------+--------------------+\n",
      "|1093225999|    $999.99|2015-03-04 00:00:...|1093225999|   Landri Fulshur|landri.fulshur@me...|(898) 198-1781,(6...|\n",
      "| 225990677|    $999.99|2014-07-11 00:00:...| 225990677|    Andrian Waite|andrian.waite@gma...|                null|\n",
      "| 197275390|    $999.99|2014-09-09 00:00:...| 197275390|    Kianu Dyneley|kianu.dyneley@gma...|                null|\n",
      "| 504736332|    $999.99|2015-01-10 00:00:...| 504736332|      Raziel Merk|raziel.merk@faceb...|(275) 456-4661,(7...|\n",
      "| 420754422|    $999.98|2015-11-23 00:00:...| 420754422|   Vishwak Farrow|vishwak.farrow@me...|(979) 784-6613,(9...|\n",
      "|1378643543|    $999.98|2014-04-04 00:00:...|1378643543|   Zasia Scrivens|zasia.scrivens@ms...|      (880) 354-8779|\n",
      "|  50874512|    $999.98|2015-03-07 00:00:...|  50874512|Samyrah Milbourne|samyrah.milbourne...|                null|\n",
      "|1009490315|    $999.98|2014-09-05 00:00:...|1009490315|Leilani Cranstoun|leilani.cranstoun...|                null|\n",
      "|2141604701|    $999.98|2014-10-18 00:00:...|2141604701|    Veida Hubbard|veida.hubbard@fac...|      (125) 967-5303|\n",
      "| 740624030|    $999.98|2014-06-22 00:00:...| 740624030|      Ori Horrage|ori.horrage@gmail...|      (587) 512-3379|\n",
      "+----------+-----------+--------------------+----------+-----------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.registerTempTable(\"trans\")\n",
    "users.registerTempTable('use')\n",
    "\n",
    "transactions.select().groupby('date')\n",
    "out=transactions.join(users, transactions.id == users.id).orderBy('amount_paid',ascending=False) \n",
    "out.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT use.name, trans.amount_paid, FROM trans,use JOIN use.id = trans.id ORDER BY trans.amount_paid DESC LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
